---
title: "GPU-Efficient Noise Techniques in Three.js"
date: 2025-04-29
draft: false
description: "Exploring various methods like pre-computed textures, procedural shaders, and vertex noise to optimize real-time noise generation in Three.js for better GPU performance."
tags: ["three.js", "webgl", "gpu", "performance", "optimization", "noise", "shaders", "procedural generation", "texture mapping"]
categories: ["web development", "graphics", "tutorial", "performance"]
custom_html: full-screen
---

<style>
  .threejs-container {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      /* Make canvas larger than viewport for visual effect */
      width: 140vw;
      display: block;
      z-index: -1;
    }

  .threejs-container canvas {
    display: block;
    width: 100%;
    height: 100%;
  }
  .content-container {
    background: rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5);
    
    position: relative;
    z-index: 1;
  }
</style>
<div class="threejs-container">
  <canvas id="threejs-canvas"></canvas>
  <script src="https://cdn.jsdelivr.net/npm/three@0.152.2/build/three.min.js"></script>
  <script>
    // === Three.js 3D Concentric Maze Background ===
    // Colors
    const BG_COLOR = new THREE.Color(0x0a103d); // darker, less saturated blue
    const MAZE_COLOR = new THREE.Color(0x8B5C2A); // brown

    // Parameters
    const RING_THICKNESS = 0.015; // Tube radius for the tori
    const MAZE_SPACING = 0.12;   // Distance between ring centers
    const NUM_RINGS = 15;        // How many rings to generate
    const CAMERA_DISTANCE_FACTOR = 0.35; // How far out from the last ring the camera starts
    const CAMERA_ANGLE_DEG =30;

    // Get canvas and set up renderer
    const canvas = document.getElementById('threejs-canvas');
    const renderer = new THREE.WebGLRenderer({ canvas, antialias: true, alpha: false }); // Enabled antialias for 3D
    renderer.setClearColor(BG_COLOR, 1);
    renderer.setPixelRatio(window.devicePixelRatio);

    // Scene
    const scene = new THREE.Scene();

    // Camera (Perspective)
    const camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 0.1, 100);
    const outerRadius = (NUM_RINGS - 1) * MAZE_SPACING;
    const cameraDistance = outerRadius * CAMERA_DISTANCE_FACTOR;
    const cameraHeight = cameraDistance * Math.sin(CAMERA_ANGLE_DEG * Math.PI / 180);
    const cameraHorizontalDist = cameraDistance * Math.cos(CAMERA_ANGLE_DEG * Math.PI / 180);

    camera.position.set(0, -cameraHorizontalDist, cameraHeight); // Position out along -Y, and up Z (Rotated 90 deg CW)
    camera.lookAt(0, 0, 0); // Look towards the center

    // Lighting
    const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
    scene.add(ambientLight);
    const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
    directionalLight.position.set(5, 10, 7.5);
    scene.add(directionalLight);

    // --- Starfield --- 
    let stars;
    function createStarfield() {
      const starCount = 5000;
      const starSpread = 50; // How far out the stars spread
      const vertices = [];
      for (let i = 0; i < starCount; i++) {
        // Spherical distribution
        const phi = Math.acos(-1 + (2 * Math.random()));
        const theta = Math.random() * 2 * Math.PI;
        const r = Math.random() * starSpread;

        const x = r * Math.sin(phi) * Math.cos(theta);
        const y = r * Math.sin(phi) * Math.sin(theta);
        const z = r * Math.cos(phi);

        vertices.push(x, y, z);
      }

      const geometry = new THREE.BufferGeometry();
      geometry.setAttribute('position', new THREE.Float32BufferAttribute(vertices, 3));

      const material = new THREE.PointsMaterial({
        color: 0xffffff,
        size: 0.6, // Increased size for brightness
        sizeAttenuation: false, // Stars stay same size regardless of distance
        depthWrite: false // Prevent stars from writing to depth buffer
      });

      stars = new THREE.Points(geometry, material);
      scene.add(stars);
    }
    createStarfield(); // Add stars to the scene

    // --- Maze Geometry and Material ---
    const mazeGroup = new THREE.Group();
    scene.add(mazeGroup);

    const vertexShader = `
      varying vec3 vWorldPosition;
      varying vec2 vUv;
      void main() {
        vec4 worldPosition = modelMatrix * vec4(position, 1.0);
        vWorldPosition = worldPosition.xyz;
        vUv = uv; // Pass UVs for potential texture use later
        gl_Position = projectionMatrix * viewMatrix * worldPosition;
      }
    `;

    const fragmentShader = `
      precision mediump float;
      uniform float u_time;
      uniform vec3 u_mazeColor;
      uniform float u_mazeSpacing;
      uniform float u_ringBaseRadius; // Base radius of this specific torus

      varying vec3 vWorldPosition;
      varying vec2 vUv;

      const float PI = 3.14159265359;

      void main() {
        // Calculate polar coordinates in XY plane from world position
        float r = length(vWorldPosition.xy); // Distance from center in XY plane
        float theta = atan(vWorldPosition.y, vWorldPosition.x);

        // Spin: rotate pattern over time
        float spin = u_time * 0.15; // slow clockwise
        theta -= spin;

        // Maze gap logic (adapted from 2D version)
        // Note: u_ringBaseRadius helps determine steps for *this* ring
        float mazeSteps = 8.0 + floor(u_ringBaseRadius / u_mazeSpacing) * 2.0;
        float mazeAngle = 2.0 * PI / mazeSteps;
        float sector = mod(theta + PI, mazeAngle);
        // Dynamic gap size based on radius and time
        float gap = 0.3 + 0.18 * sin(floor(u_ringBaseRadius / u_mazeSpacing) * 2.3 + u_time * 0.4);
        float gapAngle = mazeAngle * gap;

        // Discard fragments that fall into the calculated gap
        if (sector < gapAngle) {
          discard;
        }

        // Color output for visible parts
        gl_FragColor = vec4(u_mazeColor, 1.0);
      }
    `;

    // Create multiple tori
    const tori = [];
    for (let i = 0; i < NUM_RINGS; i++) {
      const ringRadius = i * MAZE_SPACING;
      // Skip ring 0 (center point)
      if (ringRadius <= 0.0) continue;

      const geometry = new THREE.TorusGeometry(ringRadius, RING_THICKNESS, 16, 100); // Adjust segments as needed

      // Each torus needs its own material instance ONLY IF uniforms differ per torus
      // Here, u_ringBaseRadius differs, so we need separate materials.
      const material = new THREE.ShaderMaterial({
          uniforms: {
              u_time: { value: 0 },
              u_mazeColor: { value: MAZE_COLOR },
              u_mazeSpacing: { value: MAZE_SPACING },
              u_ringBaseRadius: { value: ringRadius } // Pass the base radius
          },
          vertexShader,
          fragmentShader,
          // side: THREE.DoubleSide // Optional: Render backfaces if needed
      });

      const torus = new THREE.Mesh(geometry, material);
      mazeGroup.add(torus);
      tori.push(torus); // Store for cleanup and uniform updates
    }

    // --- Resize Handler ---
    function resizeRenderer() {
      // Use canvas dimensions, not window dimensions
      const w = canvas.clientWidth;
      const h = canvas.clientHeight;
      // Check if dimensions are valid and resizing is needed
      const needResize = canvas.width !== w || canvas.height !== h;
      if (needResize) {
          renderer.setSize(w, h, false); // Set buffer size (false = don't set style)
      }
      camera.aspect = w / h;   // Update camera aspect ratio
      camera.updateProjectionMatrix(); // Apply changes
    }

    // Initial setup and event listener
    resizeRenderer(); // Initial size
    window.addEventListener('resize', resizeRenderer);

    // --- Animation Loop ---
    let animationId;
    const clock = new THREE.Clock();

    function animate() {
      animationId = requestAnimationFrame(animate);

      const elapsedTime = clock.getElapsedTime();

      // Update time uniform for all torus materials
      tori.forEach(torus => {
          torus.material.uniforms.u_time.value = elapsedTime;
      });

      // Optional: Rotate the whole maze group slowly if desired
      // mazeGroup.rotation.z = elapsedTime * 0.05;

      renderer.render(scene, camera);
    }
    animate();

    // --- Cleanup --- 
    window.addEventListener('beforeunload', () => {
      cancelAnimationFrame(animationId);
      tori.forEach(torus => {
        torus.geometry.dispose();
        torus.material.dispose();
      });
      if (stars) { // Dispose stars if they exist
        stars.geometry.dispose();
        stars.material.dispose();
      }
      renderer.dispose();
      // Dispose lights if needed
      ambientLight.dispose();
      directionalLight.dispose();
    });
  </script>
  <script>
    function setThreejsBgHeight() {
      var bg = document.querySelector('.threejs-container');
      if (bg) {
        bg.style.height = window.innerHeight + 'px';
        bg.style.minHeight = window.innerHeight + 'px';
      }
    }
    window.addEventListener('DOMContentLoaded', setThreejsBgHeight);
    window.addEventListener('resize', setThreejsBgHeight);
    window.addEventListener('orientationchange', setThreejsBgHeight);
  </script>
</div>
<div class="content-container">

<p>Ah, a fellow traveler seeking more efficient paths in the realm of real-time rendering! You're right, calculating complex noise per-pixel every frame can be a real GPU hog. Thankfully, Three.js offers several more GPU-friendly alternatives. Let's explore some of the most common and effective approaches:</p>

<ol>
  <li>
    <p><strong>Pre-computed Noise Textures:</strong></p>
    <ul>
      <li><strong>Concept:</strong> Instead of generating noise on the fly, you calculate the noise texture once (either offline or during the scene loading) and then sample from this texture in your shaders.</li>
      <li><strong>How it works:</strong> You can use various tools or libraries (like `simplex-noise` in JavaScript) to generate a 2D or 3D noise texture. This texture is then loaded into Three.js as a `Texture` and passed as a uniform to your materials. In your shader, you use the UV coordinates (for 2D) or world position (for 3D) to sample the pre-computed noise value.</li>
      <li><strong>GPU Efficiency:</strong> This is significantly more efficient because the complex noise calculation happens only once, not per-pixel per frame. The GPU only performs a texture lookup, which is a highly optimized operation.</li>
      <li><strong>Considerations:</strong>
        <ul>
          <li><strong>Memory Usage:</strong> Larger or higher-resolution noise textures will consume more GPU memory.</li>
          <li><strong>Static Nature:</strong> The noise pattern is static unless you explicitly update the texture (which can still be less expensive than per-pixel calculation).</li>
          <li><strong>Tiling:</strong> You might need to handle tiling artifacts if your UVs or positions go beyond the texture boundaries.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Procedural Noise with Simplified Shader Functions:</strong></p>
    <ul>
      <li><strong>Concept:</strong> Implement a simpler noise algorithm directly within your shader. While still procedural, these algorithms are designed to be less computationally intensive than complex noise functions.</li>
      <li><strong>How it works:</strong> You write GLSL code in your vertex or fragment shader that generates a noise-like pattern based on input parameters like UV coordinates, world position, or time. Simpler algorithms like value noise with fewer octaves or basic gradient noise implementations can be used.</li>
      <li><strong>GPU Efficiency:</strong> More efficient than complex per-pixel noise because the calculations within the shader are streamlined.</li>
      <li><strong>Considerations:</strong>
        <ul>
          <li><strong>Quality:</strong> Simplified noise algorithms might not produce the same level of detail or visual complexity as more advanced methods.</li>
          <li><strong>Shader Complexity:</strong> While simpler, the shader will still perform calculations per-pixel. Careful optimization of the noise function is crucial.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Noise Libraries Optimized for Shaders:</strong></p>
    <ul>
      <li><strong>Concept:</strong> Utilize GLSL noise libraries that are specifically designed for GPU performance. These libraries often contain optimized implementations of various noise algorithms.</li>
      <li><strong>How it works:</strong> You include the GLSL code from these libraries directly into your shader. They provide functions you can call to generate noise based on your inputs.</li>
      <li><strong>GPU Efficiency:</strong> These libraries are often written with performance in mind, leveraging GPU-friendly operations and minimizing complex branching or calculations.</li>
      <li><strong>Considerations:</strong>
        <ul>
          <li><strong>Integration:</strong> You need to manage and include the GLSL code within your Three.js shader material.</li>
          <li><strong>Learning Curve:</strong> Understanding how to use the specific functions provided by the library might require some learning.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Using Built-in Textures and Transformations:</strong></p>
    <ul>
      <li><strong>Concept:</strong> Leverage existing texture patterns (like gradients, patterns, or even simpler noise textures) and animate or transform their UV coordinates or the object's geometry to create the illusion of dynamic noise.</li>
      <li><strong>How it works:</strong> You can use `TextureLoader` to load simple grayscale textures. In your shader, you can then manipulate the UV coordinates based on time or other parameters to make the texture appear to move, scale, or rotate, creating a dynamic effect. Alternatively, you can animate the object's vertices in the vertex shader based on time or other inputs to create wave-like or turbulent motions.</li>
      <li><strong>GPU Efficiency:</strong> This is generally very efficient as it relies on optimized texture sampling and relatively simple transformations.</li>
      <li><strong>Considerations:</strong>
        <ul>
          <li><strong>Visual Complexity:</strong> The resulting effects might be less intricate than true procedural noise.</li>
          <li><strong>Creativity:</strong> Requires clever manipulation of textures and transformations to achieve desired results.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Vertex-Based Noise and Interpolation:</strong></p>
    <ul>
      <li><strong>Concept:</strong> Calculate noise or displacement at the vertices of your geometry in the vertex shader and then let the GPU's rasterizer interpolate these values across the faces.</li>
      <li><strong>How it works:</strong> In your vertex shader, you calculate a noise value (using a pre-computed texture or a simple procedural function) based on the vertex position. You can then use this noise value to displace the vertex along its normal or in other directions. The fragment shader then interpolates these displaced positions and potentially other vertex attributes (like noise values) across the surface.</li>
      <li><strong>GPU Efficiency:</strong> Significantly reduces per-pixel calculations as the noise is only evaluated per vertex. The interpolation is handled efficiently by the GPU.</li>
      <li><strong>Considerations:</strong>
        <ul>
          <li><strong>Detail:</strong> The level of detail in the noise effect is limited by the vertex density of your geometry. Lower poly counts will result in smoother, less detailed noise.</li>
          <li><strong>Silhouette:</strong> The displacement affects the object's silhouette.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Which approach is best?</strong> It often depends on the specific requirements of your scene:</p>
<ul>
  <li>For highly detailed but mostly static noise, <strong>pre-computed textures</strong> are excellent.</li>
  <li>For simpler, dynamic patterns, <strong>simplified shader functions</strong> or <strong>texture transformations</strong> can be very effective.</li>
  <li>When visual quality and complexity are paramount but you still need better performance than raw per-pixel noise, <strong>optimized GLSL libraries</strong> or <strong>vertex-based noise</strong> are good choices.</li>
</ul>

<p>Often, a combination of techniques yields the best balance between visual fidelity and performance. Experiment and profile to see what works best for your particular use case!</p>

</div>
